---
title:
author:
date:
output: pdf_document
header-includes:
   - \usepackage{bm}
---

## Methods

We seek to estimate the probability that an encrypted vote $V$ with byte length $B$ is for candidate $k$, $\pi(V_k \vert B)$.  Bayes' rule allows us to rewrite this probability as 

$$ \pi(V_k \vert B) = \dfrac{\pi(B \vert V_k) \pi (V_k)}{ \sum _k \pi(B \vert V_k) \pi (V_k)} \>. $$

\noindent Here, $\pi(V_k)$ is known as \textit{the prior} and is interpreted as the proportion of people expected to vote for candidate $k$ prior to the election.  The quantity $\pi(B \vert V_k )$ is known as \textit{the likelihood} and can be interpreted as the probability of observing an encrypted vote of byte length $B$ for candidate $k$.  The sum in the denominator is a normalizing constant and can be ignored for our purposes, meaning $\pi(V_k \vert B) \propto \pi(B\vert V_k)\pi(V_k)$.

We classify votes according to the candidate $k$ who maximizes the posterior probability.  Mathematically, our prediction,  $\widehat{V}_k$, is

\begin{align*}
\widehat{V}_k &= \underset{k \in K}{\arg\max} \left\{  \pi(V_k \vert B)  \right\} \>, \\
& = \underset{k \in K}{\arg\max} \left\{  \pi(B\vert V_k)\pi(V_k)  \right\} \>.
\end{align*}


\noindent The likelihood, $\pi(B\vert V_k)$, is generally unknown.  However, simplifying assumptions can be used to facilitate prediction. In particular, if we consider byte length as a categorical variable rather than a numeric variable, then we can assume the likelihood for byte length is multinomial

$$ \pi(B \vert V_k) = \operatorname{Multinomial}(\bm{\theta}_k) \>. $$

\noindent Here, the multinomial parameter $\bm{\theta}_k$ is indexed by $k$ to allow for different candidates to have different probabilities for observing various byte lengths.  Making this assumption on the likelihood leads to the \textit{Multinomial Naive Bayes Model}.  Using data with labelled votes and byte lengths, the $\bm{\theta}_k$ can be estimated and then used to make predictions.  

## Model Evaluation

We perform 100 repeats of 10 fold cross validation to evaluate our model.  Briefly, $v$-fold cross validation is a technique to estimate out of sample performance of a predictive model.  Data are split into $v$ equally sized and disjoint subsets (in our case, $v=10$).  To estimate the out of sample performance, $v-1$ subsets are combined and used to fit the model.  The model is then used to predict on the remaining subset of data.  Performance metrics are calculated on these predictions.  This process is repeated until all $v$ subsets have acted as a hold out set.  The performance metrics are averaged over  the $v$ subsets.  Repeating $v$ fold cross validation 100 times is a way of avoiding spurious performance estimates based on fortuitous splits.

We evaluate model classification ability using accuracy, precision, recall, and log loss.  Precision and recall are class specific metrics, and so we compute their weighted average in our hold out sets.  Interpretation of each metric is as follows:

\begin{description}
\item[Accuracy] is the proportion of correctly identified votes.  Probabilistically, accuracy is the probability the vote is correctly identified, $\pi(\widehat{V}_k = V_k)$.

\item[Precision] is the proportion of predictions for candidate $k$ which correctly identify a vote for candidate $k$.  Probabilistically, the precision is the probability the vote is for candidate $k$ conditioned on the prediction being for candidate $k$, $\pi(V_k \vert \widehat{V}_k)$.  As an example, suppose in our sample we predict 100 votes will be for candidate $k$.  Of those 100, 72 are actually for candidate $k$.  The precision for candidate $k$ is then $0.72 = 72/100$.

\item[Recall] is the proportion of votes for candidate $k$ which are correctly predicted to be for candidate $k$.  Probabilistically, recall is the probability the prediction made is for candidate $k$ conditioned on the vote being for candidate $k$, $\pi(\widehat{V}_k \vert V_k)$.  As an example, suppose in our sample there are 100 votes for candidate $k$.  Of those 100 votes, we correctly predict that 78 votes will be for candidate $k$.  The recall is then $0.78=78/100$.		
\end{description}


We report the average accuracy, precision, and recall over the 100 repeats of 10 fold cross validation as well as 2 standard deviations.

## Experiment 1

```{r setup, echo=FALSE, message=FALSE, warning=FALSE}

library(tidyverse)
library(kableExtra)
library(patchwork)

theme_set(theme_minimal())

knitr::opts_chunk$set(
  warning = F,
  message = F,
  echo = F,
  fig.height = 5,
  fig.width = 8,
  fig.align = 'center'
)
```


```{r experiment-1-plot, fig.cap='Distribution of byte length colored by candidate. In our experiment, all candidates have an equal number of votes.  The candidates can easily be discriminated by eye, providing reassurance that a simple decision rule such as a naive bayes model may be able to effectively discriminate candidates given byte length.'}
experiment_1_data<- read_csv('data/selwyn-just-mayor.csv')

experiment_1_data %>% 
  ggplot(aes(byte_length, fill=mayor))+
  geom_histogram(color='black', position='identity', alpha = 0.5)+
  scale_fill_brewer(palette = 'Set1')+
  labs(fill='Ballot Selection', x='Byte Length', y='Count')+
  theme(aspect.ratio = 0.62)



```


```{r experiment-1-table}

experiment_1_results <- read_csv('experiment_1/Naive_Bayes_cross_validated_performance.csv')

longer_metrics <- experiment_1_results %>% 
  select(test_accuracy:test_recall) %>% 
  pivot_longer(cols = test_accuracy:test_recall, names_to = 'Metric') %>% 
  mutate(Metric = str_remove(Metric, 'test_'),
         Metric = str_to_title(Metric))


tbl_1_cap <- 'Cross validated performance for our model on our first experiment.  There are 4 candidates on the ballot for this experiement.  This means that if byte length were truly non-informative, we would expect an accuaracy, precision, and recall of 0.25.  All three metrics are well above 80\\%, meaning byte length provides information which allows us to discriminate between votes.'


longer_metrics %>% 
  group_by(Metric) %>% 
  summarise(Estimate = mean(value),
            `Standard Deviation` = sd(value)) %>% 
  kbl(digits=3, 
      caption = tbl_1_cap,
      booktabs = T, 
      position = 'h!')
```



